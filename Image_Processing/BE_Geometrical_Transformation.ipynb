{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d5308f",
   "metadata": {},
   "source": [
    "__Corresponding Teacher__: Dawood AL CHANTI, MCF, PHELMA.\n",
    "\n",
    "__Email__: dawood.al-chanti@grenoble-inp.fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c14c28",
   "metadata": {},
   "source": [
    "<center><b><font style=\"color: orange\" size=\"5\">BE: 5: Geometrical Transformation: Forward and Backward Mapping</font></b></center>\n",
    "&ensp;\n",
    "\n",
    "\n",
    "> <left><b><font style=\"color: blue\" size=\"5\"> Deadline: </font></b></center> &ensp;  <left><b><font style=\"color: red\" size=\"5\"> **31/03/2025 latest at 23h59**</font></b></center> &ensp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd1f0a",
   "metadata": {},
   "source": [
    "> <left><b><font style=\"color: blue\" size=\"3\"> Student Information: </font></b></center> &ensp;\n",
    "\n",
    "\n",
    "* Student 1:\n",
    "    - Last Name: DUPAYAGE\n",
    "    - First Name: Quentin\n",
    "    - Identifier/algan: dupayagq\n",
    "    \n",
    "* Student 2:\n",
    "    - Last Name: MELKIOR\n",
    "    - First Name: Clément\n",
    "    - Identifier/algan: melkiorc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef63674",
   "metadata": {},
   "source": [
    "> <left><b><font style=\"color: blue\" size=\"3\"> Working in pairs or alone. </font></b></center> &ensp;\n",
    "\n",
    "\n",
    "* **To upload your work, follow the following steps:**\n",
    "    - Go to https://chamilo.grenoble-inp.fr/ \n",
    "    - Go to the course `4PMSTIA5 Traitement d'images avancé`\n",
    "        - Go to the section Travaux d'étudiants\n",
    "        - Upload your work under `BE: séance 3`\n",
    "        - **You must upload this `.ipynb` as it is, which will contain your code and your comments/analysis.**\n",
    "            - **Any other formate, will not be corrected.**\n",
    "            \n",
    "            \n",
    "\n",
    "> <left><b><font style=\"color: red\" size=\"3\">Important: </font></b></center> &ensp; <center><b><font style=\"color: red\" size=\"3\"> \n",
    "    \n",
    "- **Consider delievering a clean notebook, that contain the meaningful experimental results and analysis. Remove other experiments that you performed, which you considered as just trial.**\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fac8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da972b7",
   "metadata": {},
   "source": [
    "#### Quantification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c70783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    " \n",
    "    \n",
    "def mse(img1, img2):\n",
    "    return np.mean((img1.astype(np.float32) - img2.astype(np.float32)) ** 2)\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse_val = mse(img1, img2)\n",
    "    return 20 * log10(255.0 / np.sqrt(mse_val))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60581a9b",
   "metadata": {},
   "source": [
    "# IMPORTANT TO READ TO UNDERSTAND THE CONCEPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7c1e0",
   "metadata": {},
   "source": [
    "# Understanding Image Coordinates vs Cartesian Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208ce4f",
   "metadata": {},
   "source": [
    "When working with images in computer vision, it's important to understand the **difference between image coordinates and Cartesian coordinates** (centered at origin).\n",
    "\n",
    "### Image Dimensions\n",
    "We are working with an image of:\n",
    "\n",
    "- **Width = 356 pixels**\n",
    "- **Height = 256 pixels**\n",
    "\n",
    "In **image coordinates**:\n",
    "- The origin `(0, 0)` is at the **top-left corner**\n",
    "- The x-axis goes **right**\n",
    "- The y-axis goes **down**\n",
    "\n",
    "In **Cartesian coordinates (centered)**:\n",
    "- The origin `(0, 0)` is at the **center of the image**\n",
    "- The x-axis goes **right**\n",
    "- The y-axis goes **up**\n",
    "\n",
    "---\n",
    "\n",
    "### Mapping Image to Cartesian\n",
    "\n",
    "To convert a pixel from image coordinates `(x_img, y_img)` to centered Cartesian coordinates `(x_cart, y_cart)`, we use:\n",
    "\n",
    "```python\n",
    "x_cart = x_img - (width / 2)\n",
    "y_cart = y_img - (height / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30190c74",
   "metadata": {},
   "source": [
    "* So the top-left corner (0, 0) in image coordinates becomes:\n",
    " \n",
    "\n",
    "```python\n",
    "x_cart = 0 - 178 = -178\n",
    "y_cart = 0 - 128 = -128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb624ce",
   "metadata": {},
   "source": [
    "### Relation with geometrical transoformation:\n",
    "* When performing geometric transformations (like rotation or scaling), we typically want to rotate or transform around the center of the image, not around the corner.\n",
    "\n",
    "* By shifting the origin to the center, we ensure:\n",
    "   - Rotations are centered\n",
    "   - Transformations behave intuitively\n",
    "   - Math is consistent with standard Cartesian geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "height = 256\n",
    "width = 356\n",
    "\n",
    "# Image center in image coordinates\n",
    "x_center_img = width / 2\n",
    "y_center_img = height / 2\n",
    "\n",
    "# Top-left corner in Cartesian (centered) coords\n",
    "top_left_cartesian_x = 0 - x_center_img  # -178\n",
    "top_left_cartesian_y = 0 - y_center_img  # -128\n",
    "\n",
    "# Create a canvas for display\n",
    "fig, ax = plt.subplots(figsize=(8, 9))\n",
    "ax.set_xlim(-width / 2 - 50, width / 2 + 50)\n",
    "ax.set_ylim(-height / 2 - 50, height / 2 + 50)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Draw full image rectangle centered at (0, 0)\n",
    "rect = plt.Rectangle((-width/2, -height/2), width, height,\n",
    "                     edgecolor='blue', facecolor='none', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# Mark origin\n",
    "ax.plot(0, 0, 'ro')\n",
    "ax.text(-15, -10, 'Origin (0, 0)', color='red', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Mark top-left corner\n",
    "ax.plot(top_left_cartesian_x, top_left_cartesian_y, 'go')\n",
    "ax.text(top_left_cartesian_x -10, top_left_cartesian_y -10,\n",
    "        f'Top-Left (Image: 0,0)\\nCartesian: ({int(top_left_cartesian_x)}, {int(top_left_cartesian_y)})',\n",
    "        color='green', fontsize=11)\n",
    "\n",
    "# Draw arrows for X and Y axes to represent directions\n",
    "arrow_len = 60\n",
    "ax.arrow(0, 0, arrow_len, 0, head_width=10, head_length=10, fc='black', ec='black')\n",
    "ax.arrow(0, 0, 0, arrow_len, head_width=10, head_length=10, fc='black', ec='black')\n",
    "\n",
    "# Labels for directions (shifted for clarity)\n",
    "ax.text(arrow_len + 15, 5, 'X →', fontsize=12, color='black', verticalalignment='bottom')\n",
    "ax.text(5, arrow_len + 15, 'Y ↓', fontsize=12, color='black', horizontalalignment='left')\n",
    "\n",
    "# Flip Y-axis to match image coordinate system (top-left origin)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Axes and labels\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_title(\"Image Coordinate (0,0) Mapped to Centered Cartesian Domain\", fontsize=14)\n",
    "ax.set_xlabel(\"X (centered)\")\n",
    "ax.set_ylabel(\"Y (centered)\")\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f617d",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10debe",
   "metadata": {},
   "source": [
    "# Session starts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdc73c",
   "metadata": {},
   "source": [
    "### Load and Display Image called `parrot.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbae600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grayscale image\n",
    "image = cv2.imread(\"parrot.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get image dimensions\n",
    "rows, cols = image.shape\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Affichage des lignes de l'image\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2d114",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed76c0f",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fce396",
   "metadata": {},
   "source": [
    "### Load and Display ground truth (transformed image), it is saved as `ground_truth.npy'\n",
    "   * You will use this image as your ground truth to quantify and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ed688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved .npy file fot that use np.load\n",
    "loaded_gt = np.load(\"ground_truth.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bda35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Ground Truth Image image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(loaded_gt, cmap='gray')\n",
    "plt.title(\"Affichage des lignes de l'image\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae4d84",
   "metadata": {},
   "source": [
    "### Define a   transformation matrix `T`  based on the following transformation parameters:\n",
    "\n",
    "   * `angle_deg` which define the amount of rotaion.\n",
    "   * `tx` which define the amount of translation in `x` direction.\n",
    "   * `ty` which define the amount of translation in `y` direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf590f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_deg = 30.6        \n",
    "tx = 22.3               \n",
    "ty = 10.15             \n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "\n",
    "#Build 3x3 affine transformation matrix T  in homogenous domain (clockwise)\n",
    "T = np.array([\n",
    "    [cos_a, -sin_a, tx],\n",
    "    [sin_a, cos_a, ty],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c071b",
   "metadata": {},
   "source": [
    "## Q. Can you explain what this transformation does to the spatial structure of the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca318df",
   "metadata": {},
   "source": [
    "Ca tourne l'image dans le sens horaire de 30.6 ° et la translate sur l'axe x de Dx = 22.3 pixels, sur l'axe y de Dy = 10.15 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f6317",
   "metadata": {},
   "source": [
    "# Experiment 1: Forward Mapping with NN interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59984753",
   "metadata": {},
   "source": [
    "## Perform Forward Mapping with Nearest Neighbor Interpolation. To do that, rely on the pseudo code given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fecf1e",
   "metadata": {},
   "source": [
    "### Pseudocode: Forward Mapping with NN Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8196a",
   "metadata": {},
   "source": [
    "Input: \n",
    "   - image: 2D grayscale image\n",
    "   - T: 3x3 transformation matrix (rotation + translation)\n",
    "\n",
    "Output:\n",
    "   - `output_forward_nn`: transformed image.\n",
    "\n",
    "1. Get the number of rows and columns in the image\n",
    "\n",
    "2. Create a new output image filled with zeros (same size as input) call it `output_forward_nn`\n",
    "\n",
    "3. For each pixel (x, y) in the input image:\n",
    "    a. Subtract image center to get centered coordinates:\n",
    "        x_centered = ###\n",
    "        y_centered = ###\n",
    "\n",
    "    b. Apply the transformation T to the centered coordinates:\n",
    "        new_x = ### * ### + ### * ### + ###\n",
    "        new_y = ### * ### + ### * ### + ###\n",
    "\n",
    "    c. Shift the new coordinates back to image space:\n",
    "        new_x = ### + ###\n",
    "        new_y = ### + ###\n",
    "\n",
    "    d. Round new_x and new_y to the nearest integers simply using ``int(round(value))``\n",
    "        new_x_int = ###\n",
    "        new_y_int = ###\n",
    "\n",
    "    e. If the new coordinates `(new_x_int,new_y_int)` are inside the image bounds `(0 <= new_x_int < cols` and `0 <= new_y_int < rows)`: \n",
    "        - Copy the pixel value from (x, y) in the original image to (new_x_int, new_y_int) in the output image `output_forward_nn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc4041",
   "metadata": {},
   "source": [
    "### Display the output image and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326919a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_forward_nn (image, T):\n",
    "    \"\"\"\n",
    "    Transformation method in order to apply structural transformation on an image\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (NumPy array)\n",
    "    - T: Matrix of the structural transformation\n",
    "\n",
    "    Returns:\n",
    "    - Transformed image\n",
    "    \"\"\"\n",
    "    # 1. Récupérer le nombre de ligne et de colonne, ainsi que les centres\n",
    "    rows, cols = image.shape\n",
    "    center_x = cols / 2\n",
    "    center_y = rows / 2\n",
    "\n",
    "    # 2. Créer l'image de sortie \n",
    "    output_forward_nn = np.zeros_like(image)\n",
    "    \n",
    "    # 3. Parcourir chaques pixels de l'image de départ\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            # a. Coordonées centrées\n",
    "            x_centered = x - center_x\n",
    "            y_centered = y - center_y\n",
    "\n",
    "            # b. Appliquer la transformation\n",
    "            new_x = T[0, 0] * x_centered + T[0, 1] * y_centered + T[0, 2]\n",
    "            new_y = T[1, 0] * x_centered + T[1, 1] * y_centered + T[1, 2]\n",
    "\n",
    "            # c. Revenir à l'espace image\n",
    "            new_x += center_x\n",
    "            new_y += center_y\n",
    "\n",
    "            # d. Arrondir aux entiers les plus proches\n",
    "            new_x_int = int(round(new_x))\n",
    "            new_y_int = int(round(new_y))\n",
    "\n",
    "            # e. Vérifier si les nouvelles coordonnées sont dans les limites\n",
    "            if 0 <= new_x_int < cols and 0 <= new_y_int < rows:\n",
    "                output_forward_nn[new_y_int, new_x_int] = image[y, x]\n",
    "\n",
    "    return output_forward_nn\n",
    "    \n",
    "# Apply the transformation on the image\n",
    "output_forward_nn = out_forward_nn(image, T)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Show the forward mapped image\n",
    "axes[0].imshow(output_forward_nn, cmap='gray')\n",
    "\n",
    "# Show the loaded ground truth image\n",
    "axes[1].imshow(loaded_gt, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b828ce",
   "metadata": {},
   "source": [
    "Nous remarquons que l'image reconstruite n'est pas nette, il y a quelques aspéritées: Lors de la construction de notre image transformée, nous partons d'une matrice \"np.zeros_like(image)\" (étape 2). Lors de l'attribution de  nos \"new_x_int\" et \"new_y_int\", nous appliquons l'arrondis ainsi que la conversion en entier. Ainsi, certaines positions de pixels ne sont pas atteintes, et par conséquent pas attribuées, elles restent par conséquent à 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114554e1",
   "metadata": {},
   "source": [
    "# Quantifying the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb92b53",
   "metadata": {},
   "source": [
    "You are provided with a set of quantitative metrics to help evaluate the difference between the ground truth and the transformed image you obtained.\n",
    "---\n",
    "\n",
    "### 1. **MSE (Mean Squared Error)** \n",
    "The average squared difference between each pixel in the transformed image and the corresponding pixel in the ground truth image.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\\\[\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( I_{\\text{gt}}[i] - I_{\\text{output}}[i] \\right)^2\n",
    "\\\\]\n",
    "- Lower MSE = better quality\n",
    "- Sensitive to large pixel differences\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **PSNR (Peak Signal-to-Noise Ratio)**\n",
    "A logarithmic measurement of the peak possible pixel value vs. the observed error (MSE).\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\\\[\n",
    "\\text{PSNR} = 20 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}_I}{\\sqrt{\\text{MSE}}} \\right)\n",
    "\\\\]\n",
    "\n",
    "Where \\\\(\\text{MAX}_I = 255\\\\) for 8-bit grayscale images.\n",
    "- Higher PSNR = better image quality\n",
    "---\n",
    "\n",
    "### 3. **SSIM (Structural Similarity Index)**\n",
    "SSIM compares luminance, contrast, and structure between the two images — closer to how humans perceive visual quality.\n",
    "\n",
    "**Range:**  \n",
    "\\\\[\n",
    "0 \\leq \\text{SSIM} \\leq 1\n",
    "\\\\]\n",
    "\n",
    "- SSIM closer to 1 = higher structural similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53615e",
   "metadata": {},
   "source": [
    "### Compute the mean square error using the provided function `mse()`, Peak Signal-to-Noise Ratio using `psnr()` and Structural Similarity Index ssim()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_valFNN = mse(output_forward_nn, loaded_gt)\n",
    "psnr_valFNN = psnr(output_forward_nn, loaded_gt)\n",
    "ssim_valFNN = ssim(output_forward_nn, loaded_gt)\n",
    "\n",
    "\n",
    "#Given:\n",
    "print(f\"{'Method':<20} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Forward NN':<20} | {mse_valFNN:10.2f} | {psnr_valFNN:10.2f} | {ssim_valFNN:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f86d65",
   "metadata": {},
   "source": [
    "## Quantitative Result Analysis\n",
    "\n",
    "After applying **Forward Mapping (Nearest Neighbor)**, you obtained the evaluation metrics by comparing your output with the ground truth image:\n",
    "\n",
    "### Questions for Analysis\n",
    "\n",
    "1. **Mean Squared Error (MSE)**  \n",
    "   - What does MSE measure in this context?\n",
    "   - What could cause the MSE to be higher?\n",
    "\n",
    "2. **Peak Signal-to-Noise Ratio (PSNR)**  \n",
    "   - What does PSNR tell you about image quality?\n",
    "   - Generally, PSNR > 30 dB is considered high quality. How does your result compare?\n",
    "   - What does a low PSNR may indicate in image transformation tasks?\n",
    "\n",
    "3. **Structural Similarity Index (SSIM)**  \n",
    "   - SSIM ranges from 0 to 1. What does your SSIM value indicate?\n",
    "   - Does it reflect high structural similarity to the original image?\n",
    "\n",
    "---\n",
    "\n",
    "###  Reflection Analysis\n",
    "- Forward mapping can leave holes or unassigned pixels in the output image as you observed.\n",
    "  - How could this affect the MSE and SSIM?\n",
    "  - Why is backward mapping often preferred in practical applications?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2d019",
   "metadata": {},
   "source": [
    "MSE = mean square error: mesure la moyenne de la différence au carré entre chaques pixels de notre image transformée avec la ground truth image. Ainsi, plus il y aura de différence entre chaques pixels des deux images, au plus notre MSE sera important (ici 2015.57 est un gros MSE, résultant du fait que de nombreux pixels sont à 0)\n",
    "PSNR = Peak Signal-to-Noise Ratio: Mesure logarithmique du rapport entre l'intensité maximal de l'image et la racine carré du MSE de l'image. Ainsi, deux facteurs rentrent en jeux pour cette mesure: avoir une importante luminosité dans l'image (pour une image en niveau de gris, max_I = 255), ainsi qu'un MSE faible relativement faible par rapport à l'intensité de l'image. Nous pouvons remarquer qu'au plus une image aura une intensité lumineuse importante, au plus Max_I sera important, mais au plus le MSE sera important s'il y a des \"dead-pixels\" (pixels à 0), ainsi ces deux facteurs sont tout de même relié. Un PSNR > 30 indique que nous devons avoir un rapport (Max_I/sqrt(MSE)) minimum de 4.48, ce qui n'est pas notre cas, nous obtenons un PSNR de 15.09 due à notre MSE important par rapport à l'intensité générale de l'image. Cela indique que nous avons trop de dead-pixels dans notre image transformée.\n",
    "SSIM = Structural Similitude Index: Notre SSIM est relativement bas (0.2883) due surtout au contraste et la luminosité entre les 2 images. Les trou (dead-pixels) créaient trop de constraste entre les deux image, baissant la luminosité générale de l'image et ainsi la similitude détecté entre les structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff17af7",
   "metadata": {},
   "source": [
    "Comme expliqué précédemment, la création de \"dead-pixels\" dans l'image va engendré de plus grosse erreur sur certains pixels, ce qui va énormément faire croître la MSE, ainsi que la contraste entre les deux image, ce qui affecte énormément la MSE (MSE va être plus importante) ainsi que la SSIM (SSIM va être plus proche de 0).\n",
    "L'avantage du backward mapping est que l'on ne créer pas de trou: pour chaques pixels de l'image de départ (loaded_gt), nous attribuons un pixel de l'image d'arrivée (image). Chaques pixels a alors une valeur non-nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build 3x3 affine transformation matrix T2 in homogenous domain (counter clockwise)\n",
    "T2 = np.array([\n",
    "    [cos_a, sin_a, tx],\n",
    "    [-sin_a, cos_a, ty],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "def out_backward_nn(image, T):\n",
    "    \"\"\"\n",
    "    Transformation method in order to apply structural transformation on an image\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (NumPy array)\n",
    "    - T: Matrix of the structural transformation\n",
    "\n",
    "    Returns:\n",
    "    - Transformed image\n",
    "    \"\"\"\n",
    "    # 1. Récupérer les lignes et les colonnes, calculer les centres\n",
    "    rows, cols = image.shape\n",
    "    center_x, center_y = cols / 2, rows / 2\n",
    "\n",
    "    # 2. Créer l'image et calcul l'inverse de la transformation\n",
    "    output_image = np.zeros_like(image)\n",
    "    T_inv = np.linalg.inv(T)\n",
    "\n",
    "    # 3. Pour chaques pixels\n",
    "    for y_out in range(rows):\n",
    "        for x_out in range(cols):\n",
    "            # a. Coordonées centrées de l'image de sortie\n",
    "            x_centered_out = x_out - center_x\n",
    "            y_centered_out = y_out - center_y\n",
    "\n",
    "            # b. Appliquer la transformation inverse\n",
    "            src_x = T_inv[0,0]*x_centered_out + T_inv[0,1]*y_centered_out + T_inv[0,2]\n",
    "            src_y = T_inv[1,0]*x_centered_out + T_inv[1,1]*y_centered_out + T_inv[1,2]\n",
    "            x_in, y_in = src_x + center_x, src_y + center_y\n",
    "\n",
    "            # c. Arrondir pour obtenir le plus proche voisin\n",
    "            x_nearest = int(round(x_in))\n",
    "            y_nearest = int(round(y_in))\n",
    "\n",
    "            # d. Vérifier si on est dans les bornes de l'image d'origine\n",
    "            if 0 <= x_nearest < cols and 0 <= y_nearest < rows:\n",
    "                output_image[y_out, x_out] = image[y_nearest, x_nearest]\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b76ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation on the image\n",
    "output_backward_nn = out_backward_nn(loaded_gt, T2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Show the backward mapped loaded ground truth image\n",
    "axes[0].imshow(output_backward_nn, cmap='gray')\n",
    "\n",
    "# Show the original image\n",
    "axes[1].imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b79983",
   "metadata": {},
   "source": [
    "Nous voyons que l'image est bien plus nette, et qu'il n'y a pas de trou.\n",
    "(Je n'avais pas vu que nous allions l'implémenter plus bas, je voulais simplement visualiser la différence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a20de",
   "metadata": {},
   "source": [
    "# Experiment 2: Forward Mapping with Bilinear Interpolation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import map_coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b5b7f",
   "metadata": {},
   "source": [
    "#### Forward Mapping Pseudocode with Bilinear Interpolation.\n",
    "* Here we will use the function `map_coordinates()` to perform the bilinear interpolation as it is optimized and fast. However, if you are interested in details, refer to the course slide and here is the related python code :\n",
    "\n",
    "```python\n",
    "if 0 <= new_x < cols - 1 and 0 <= new_y < rows - 1:\n",
    "    x0 = int(np.floor(new_x))\n",
    "    y0 = int(np.floor(new_y))\n",
    "    x1 = x0 + 1\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # Compute weights\n",
    "    dx = new_x - x0\n",
    "    dy = new_y - y0\n",
    "\n",
    "    # Pixel contributions\n",
    "    w00 = (1 - dx) * (1 - dy)\n",
    "    w01 = dx * (1 - dy)\n",
    "    w10 = (1 - dx) * dy\n",
    "    w11 = dx * dy\n",
    "\n",
    "    # Distribute intensity using bilinear weights\n",
    "    output_forward_nn[y0, x0] += w00 * image[y, x]\n",
    "    output_forward_nn[y0, x1] += w01 * image[y, x]\n",
    "    output_forward_nn[y1, x0] += w10 * image[y, x]\n",
    "    output_forward_nn[y1, x1] += w11 * image[y, x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557208d0",
   "metadata": {},
   "source": [
    "## Pseudocode to implement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060ef7c",
   "metadata": {},
   "source": [
    "Input: \n",
    "   - image: 2D grayscale image\n",
    "   - T: 3x3 transformation matrix (rotation + translation)\n",
    "\n",
    "Output:\n",
    "   - `output_forward_nn_bilinear`: transformed image.\n",
    "\n",
    "1. Get the number of rows and columns in the image\n",
    "\n",
    "2. Create a new output image filled with zeros (same size as input) call it `output_forward_nn_bilinear`\n",
    "\n",
    "3. For each pixel (x, y) in the input image:\n",
    "    a. Subtract image center to get centered coordinates:\n",
    "        x_centered = ###\n",
    "        y_centered = ###\n",
    "\n",
    "    b. Apply the transformation T to the centered coordinates:\n",
    "        new_x = ### * ### + ### * ### + ###\n",
    "        new_y = ### * ### + ### * ### + ###\n",
    "\n",
    "    c. Shift the new coordinates back to image space:\n",
    "        new_x = ### + ###\n",
    "        new_y = ### + ###\n",
    "\n",
    "    d. If the new coordinates `(new_x,new_y)` are inside the image bounds `(0 <= new_x < cols` and `0 <= new_y < rows)`: \n",
    "      \n",
    "            Use scipy to interpolate at subpixel position\n",
    "            interpolated_value = map_coordinates(\n",
    "                image,\n",
    "                [[###], [###]],      # Coordinates in (row, col) order\n",
    "                order=1              # 1 = bilinear, 3 = cubic spline\n",
    "            )[0]\n",
    "\n",
    "            output_forward_nn_bilinear[###, ###] = ###\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_deg = -30.6        \n",
    "tx = -22.3               \n",
    "ty = -10.15             \n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "\n",
    "#Build 3x3 affine transformation matrix T  in homogenous domain (clockwise)\n",
    "T = np.array([\n",
    "    [cos_a, -sin_a, tx],\n",
    "    [sin_a, cos_a, ty],\n",
    "    [0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d08af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_bilinear (image, T, order = 1):\n",
    "    \"\"\"\n",
    "    Transformation method in order to apply structural transformation on an image\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (NumPy array)\n",
    "    - T: Matrix of the structural transformation\n",
    "    - order: Integer, parameter of map_coordinates function, default = 1\n",
    "\n",
    "    Returns:\n",
    "    - Transformed image\n",
    "    \"\"\"\n",
    "    # 1. Récupérer le nombre de ligne et de colonne, ainsi que les centres\n",
    "    rows, cols = image.shape\n",
    "    center_x = cols / 2\n",
    "    center_y = rows / 2\n",
    "\n",
    "    # 2. Créer l'image de sortie \n",
    "    output_forward_bilinear = np.zeros_like(image)\n",
    "    \n",
    "    # 3. Parcourir chaques pixels de l'image de départ\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            # a. Coordonées centrées\n",
    "            x_centered = x - center_x\n",
    "            y_centered = y - center_y\n",
    "\n",
    "            # b. Appliquer la transformation\n",
    "            new_x = T[0, 0] * x_centered + T[0, 1] * y_centered + T[0, 2]\n",
    "            new_y = T[1, 0] * x_centered + T[1, 1] * y_centered + T[1, 2]\n",
    "\n",
    "            # c. Revenir à l'espace image\n",
    "            new_x += center_x\n",
    "            new_y += center_y\n",
    "\n",
    "            # d. Vérifier si les nouvelles coordonnées sont dans les limites\n",
    "            if 0 <= new_x < cols and 0 <= new_y < rows:\n",
    "                interpolated_value = map_coordinates(\n",
    "                image,\n",
    "                [[new_y], [new_x]],      # Coordinates in (row, col) order\n",
    "                order= order              # 1 = bilinear, 3 = cubic spline\n",
    "                )[0]\n",
    "                output_forward_bilinear[y,x] = interpolated_value\n",
    "    return output_forward_bilinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583c030",
   "metadata": {},
   "source": [
    "La méthode cubic spline nécessite trop de temps de calculs pour être réaliser (sans y passer 1 h) et nous n'obtiendrions pas de tellement meilleurs résultats: l'image serait encore plus flouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90834c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward method with bilinear interpolation and order = 1\n",
    "output_forward_bilinear = forward_bilinear(image, T)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 8))\n",
    "\n",
    "# Show the forward mapped image\n",
    "axes[0].imshow(output_forward_nn, cmap='gray')\n",
    "axes[0].set_title(\"Forward Mapping: Transformed Image (NN operator)\")\n",
    "\n",
    "# Show the forward mapped image\n",
    "axes[1].imshow(output_forward_bilinear, cmap='gray')\n",
    "axes[1].set_title(\"Forward Mapping: Transformed Image (Bilinear operator)\")\n",
    "\n",
    "# Show the loaded ground truth image\n",
    "axes[2].imshow(loaded_gt, cmap='gray')\n",
    "axes[2].set_title(\"Ground Truth Image\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "mse_valFBiL = mse(output_forward_bilinear,loaded_gt)\n",
    "psnr_valFBiL = psnr(output_forward_bilinear,loaded_gt)\n",
    "ssim_valFBiL = ssim(output_forward_bilinear,loaded_gt)\n",
    "    \n",
    "#Given:    \n",
    "print(f\"{'Method':<20} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Forward bilinear':<20} | {mse_valFBiL:10.2f} | {psnr_valFBiL:10.2f} | {ssim_valFBiL:10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9228a3",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "As in the previous experiment, evaluate the results both **qualitatively** and **quantitatively**. Provide your observations on the output image in terms of:\n",
    "\n",
    "- Visual quality  \n",
    "- Structure preservation  \n",
    "- Any visible distortions  \n",
    "\n",
    "Then, compare these results with those obtained in the **first experiment**. Highlight key differences in terms of:\n",
    "\n",
    "- Accuracy  \n",
    "- Interpolation smoothness  \n",
    "- Artifact presence  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Quantitative Evaluation\n",
    "\n",
    "Compute and report the following metrics between the **original ground truth image** and the **transformed (mapped-back) result**:\n",
    "\n",
    "\n",
    "- **MSE (Mean Squared Error)**  \n",
    "- **PSNR (Peak Signal-to-Noise Ratio)**  \n",
    "- **SSIM (Structural Similarity Index)**  \n",
    "\n",
    "| Method                     | MSE       | PSNR (dB) | SSIM     |\n",
    "|----------------------------|-----------|-----------|----------|\n",
    "| Nearest Neighbor           | 2015.57   | 15.09     | 0.2883   |\n",
    "| Bilinear Interpolation     | 1247.23   | 17.17     | 0.6183   |\n",
    "\n",
    "> _Replace the placeholders with your actual computed results.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Qualitative Evaluation\n",
    "\n",
    "#### Visual Inspection Checklist:\n",
    "\n",
    "- **Sharpness** of details  \n",
    "- Presence or absence of **aliasing or blocky artifacts**  \n",
    "- How well **edges and fine structures** are preserved  \n",
    "- Any **blurring** introduced due to interpolation  \n",
    "\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Summarize your findings here. Comment on which method is more suitable and in what context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ee890",
   "metadata": {},
   "source": [
    "Pour l'évaluation quantitative, nous observons de bien meilleurs résultats avec l'interpolation bilinéaire (meilleur MSE, car plus bas, résultant sur un meilleur PSNR et surtout un bien meilleur indice de similarité de structure). L'interpolation par NN (Nearest Neighbor) créer des 'dead-pixels' qui augmente énormément le MSE, alors que l'interpolation bilinéaire attribut tous les pixels de départ, ce qui évite les trou, ainsi que les superpositions de pixels. Bien que l'image soit de bien meilleure qualité, nette, et sans aspéritées, elle n'a cependant pas un MSE nul: c'est due au fait que nous attribuons des valeurs à des pixels \"flottants\" (et non entier), et que l'opération de bilinéarité fait intervenir les valeurs pondérées des pixes voisins, ce qui modifie la valeur du pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a5e87",
   "metadata": {},
   "source": [
    "Pour l'évalutation qualitative, nous observons également de bien meilleurs résultats avec l'interpolation bilinéaire (MSE plus faible, meilleure correspondance structurelle). Comme dis précédemment, l'interpolation NN va alors créer les trou dans l'image, ce qui la rend beaucoup moins nette. Cependant, alors que l'interpolation NN ne modifie pas la valeur des pixels, l'interpolation bilinéaire introduit une pondération des pixels représentés par les 4 plus proches voisins, ce qui peut avoir un effet de \"blurring\" sur les pixels, notament visible sur le bord de l'image, où certains pixel perdent de leurs intensités due au bord noir. Une solution pour réduire cet effet serait d'utiliser \"order=3\" (ou cubic spline), qui prend en compte plus de voisins, et ainsi adoucie encore plus cet effet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dead684",
   "metadata": {},
   "source": [
    "Ainsi, l'interpolation par NN permet de conserver les valeurs exactes des pixels, mais créée des trou (dead-pixels), alors que l'interpolation bilinéaire restitue une meilleure image au globale, mais modifie la valeur des pixels due à la pondération."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b4772",
   "metadata": {},
   "source": [
    "Ainsi, lorsque nous voudrons être plus précis sur l'image, avec une restitution nette de l'image (imagerie médicale, image satélite, cinéma), la méthode par interpolation bilinéaire sera préférable (meilleur MSE et surtout meilleur SSIM), et lorsque nous voudrons plus faire de la classification, nous utiliserons la méthode par interpolation NN afin de garder au maximum le placement des pixels ainsi que leurs valeurs réelles (et non pondérées)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb5adf",
   "metadata": {},
   "source": [
    "# Experiment 3: backward Mapping with NN Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d1de9",
   "metadata": {},
   "source": [
    "### Backward Mapping With Nearest Neighbor Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd4cb1",
   "metadata": {},
   "source": [
    "# Reminder\n",
    "\n",
    "\n",
    "* **Forward Mapping**: Map each input pixel to its destination in the output using the forward transformation.\n",
    "* For each pixel in the input image:\n",
    "\n",
    "   - Compute where it should go in the output.\n",
    "   - Move that pixel to its new position.\n",
    "\n",
    "```python\n",
    "for each pixel (x, y) in input_image:\n",
    "    (dst_x, dst_y) = forward_transform(x, y)\n",
    "    output_image[dst_x, dst_y] = input_image[x, y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9252ed",
   "metadata": {},
   "source": [
    "* **Backward Mapping** (Inverse Mapping): Map each output pixel back to its source in the input image using the inverse transformation.\n",
    "* For each pixel in the output image:\n",
    "   - Compute where it came from in the input.\n",
    "   - Copy that source pixel into the current output location.\n",
    "   \n",
    "\n",
    "```python\n",
    "for each pixel (x, y) in output_image:\n",
    "    (src_x, src_y) = inverse_transform(x, y)\n",
    "    output_image[x, y] = input_image[src_x, src_y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e5eec",
   "metadata": {},
   "source": [
    "### Pseudocode: Backward Mapping with NN Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643b224",
   "metadata": {},
   "source": [
    "Input: \n",
    "    - Original image\n",
    "    - Inverse transformation matrix T_inv\n",
    "    - Image dimensions: rows, cols\n",
    "\n",
    "Output: \n",
    "    - Transformed output image `output_backward_nn`\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Get the number of rows and columns in the output image.\n",
    "\n",
    "\n",
    "2. Initialize an empty output image of the same size as the input  `output_backward_nn`.\n",
    "\n",
    "3. For each pixel (x, y) in the output image:\n",
    "    \n",
    "    a. Center the pixel coordinates around the image center:\n",
    "        x_centered = ###\n",
    "        y_centered = ###\n",
    "\n",
    "    b. Apply the inverse transformation to get source coordinates:\n",
    "        src_x = ### * ### + ### * ### + ###\n",
    "        src_y = ### * ### + ### * ### + ###\n",
    "\n",
    "    c. Shift the source coordinates back to image space:\n",
    "        src_x = ### + ###\n",
    "        src_y = ### + ###\n",
    "\n",
    "    d. Use nearest neighbor interpolation  simply using ``int(round(value))``:\n",
    "        src_x_nn = ###\n",
    "        src_y_nn = ###\n",
    "\n",
    "    e. If (src_x_nn, src_y_nn) is inside image bounds `(0 <= src_x_nn < cols) and (0 <= src_y_nn < rows)`:\n",
    "        Assign the pixel:\n",
    "            output[y, x] = input[src_y_nn, src_x_nn]\n",
    "\n",
    "4. Display or return the output image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14f34c",
   "metadata": {},
   "source": [
    "### Given: here is the transofrmation parameters and the transformation matrix you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38698268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation parameters\n",
    "angle_deg = 30.6       # Rotation angle in degrees\n",
    "tx = 22.3              # Translation along x\n",
    "ty = 10.15             # Translation along y\n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "T = np.array([\n",
    "    [cos_a, sin_a, tx],\n",
    "    [-sin_a, cos_a, ty],\n",
    "    [0,0,1]\n",
    "]) \n",
    "\n",
    "T2 = np.array([\n",
    "    [cos_a, sin_a, -tx],\n",
    "    [-sin_a, cos_a, -ty],\n",
    "    [0,0,1]\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bc3ea",
   "metadata": {},
   "source": [
    "The function is already coded, i defined it in the experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0efabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_backward_nn = out_backward_nn(loaded_gt,T)\n",
    "output_backward_nn_2 = out_backward_nn(loaded_gt,T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec1f7",
   "metadata": {},
   "source": [
    "### Display the Ground truth `loaded_gt`, the `output_forward_nn` and  the new output `output_backward_nn` in a subplot. Observe and comment on the visual quanlity of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bc922",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Create a 1x5 grid of subplots\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 6))\n",
    "\n",
    "# Forward Mapping\n",
    "axes[0].imshow(output_forward_nn, cmap='gray')\n",
    "axes[0].set_title(\"Output_forward_NN\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Ground Truth\n",
    "axes[1].imshow(loaded_gt, cmap='gray')\n",
    "axes[1].set_title(\"loaded_ground_truth\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Original Image\n",
    "axes[2].imshow(image, cmap='gray')\n",
    "axes[2].set_title(\"Original Image\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Backward Mapping\n",
    "axes[3].imshow(output_backward_nn, cmap='gray')\n",
    "axes[3].set_title(\"Output_backward_NN\")\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "# Backward Mapping (reverse translation)\n",
    "axes[4].imshow(output_backward_nn_2, cmap='gray')\n",
    "axes[4].set_title(\"Output_backward_NN with reverse translation\")\n",
    "axes[4].axis(\"off\")\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2cbef0",
   "metadata": {},
   "source": [
    "Comme nous le voyons, notre image \"backward\" ne présente aucunes aspérités, et représente très bien les structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58583e",
   "metadata": {},
   "source": [
    "### Quantify the results of `output_backward_nn`` with respect to the ground truth image `image_gt` using the functions `mse`, `psnr`, and `ssim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure with the loaded_gt\n",
    "mse_valBNN = mse(output_backward_nn,loaded_gt)\n",
    "psnr_valBNN = psnr(output_backward_nn,loaded_gt)\n",
    "ssim_valBNN = ssim(output_backward_nn,loaded_gt)\n",
    "\n",
    "# Measure with the original image\n",
    "mse_valBNN_I = mse(output_backward_nn_2,loaded_gt)\n",
    "psnr_valBNN_I = psnr(output_backward_nn_2,loaded_gt)\n",
    "ssim_valBNN_I = ssim(output_backward_nn_2,loaded_gt)\n",
    "\n",
    "print(f\"{'Method':<21} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'backward NN (tx/ty)':<21} | {mse_valBNN:10.2f} | {psnr_valBNN:10.2f} | {ssim_valBNN:10.4f}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'backward NN (-tx/-ty)':<21} | {mse_valBNN_I:10.2f} | {psnr_valBNN_I:10.2f} | {ssim_valBNN_I:10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe01c8fc",
   "metadata": {},
   "source": [
    "Nous avons essayer de décaler de -tx/-ty car nous trouvions que l'image d'output était meilleure, mais pas d'après les mesures, nous l'avons laisser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c8764",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "As in the previous experiment, evaluate the results both **qualitatively** and **quantitatively**. Provide your observations on the output image in terms of:\n",
    "\n",
    "- Visual quality  \n",
    "- Structure preservation  \n",
    "- Any visible distortions  \n",
    "\n",
    "Then, compare these results with those obtained in the **first and second experiment**. Highlight key differences in terms of:\n",
    "\n",
    "- Accuracy  \n",
    "- Interpolation smoothness  \n",
    "- Artifact presence  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Quantitative Evaluation\n",
    "\n",
    "Compute and report the following metrics between the **original ground truth image** and the **transformed (mapped-back) result**:\n",
    "\n",
    "\n",
    "- **MSE (Mean Squared Error)**  \n",
    "- **PSNR (Peak Signal-to-Noise Ratio)**  \n",
    "- **SSIM (Structural Similarity Index)**  \n",
    "\n",
    "\n",
    "> _Replace the placeholders with your actual computed results.\n",
    "\n",
    "| Method                  | MSE       | PSNR (dB) | SSIM     |\n",
    "|-------------------------|-----------|-----------|----------|\n",
    "| Forward Mapping NN      | 2015.57   | 15.09     | 0.2883   |\n",
    "| Forward Mapping Bilinear| 1247.23   | 17.17     | 0.6183   |\n",
    "| Backward Mapping NN     | 6050.76   | 10.31     | 0.4077   |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Qualitative Evaluation\n",
    "\n",
    "#### Visual Inspection Checklist:\n",
    "\n",
    "- **Sharpness** of details  \n",
    "- Presence or absence of **aliasing or blocky artifacts**  \n",
    "- How well **edges and fine structures** are preserved  \n",
    "- Any **blurring** introduced due to interpolation  \n",
    "\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Summarize your findings here. Comment on which method is more suitable and in what context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682824aa",
   "metadata": {},
   "source": [
    "Du fait que de nombreux pixels sont noir (due à la perte de d'information des coins de l'image 'loaded_gt'), le MSE (influant sur le PSNR) est très élevé. Pour rappel, le MSE mesurant, pour chaques pixels, la différence entre les deux images, va alors énormément croître. Malgrès celà, l'indice de similitude (SSIM) est supérieur à la méthode de forward mapping avec l'interpolation NN: en effet, comme l'image est nette et que tous les pixels sont attribués, la similitude entre les deux objets est plus importante que l'image contenant des \"trou\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e7b13",
   "metadata": {},
   "source": [
    "# Experiment 4: backward Mapping with Bilinear Interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78884438",
   "metadata": {},
   "source": [
    "### Modify the pseudocode implementation of experiment 3 in `Backward Mapping with NN Interpolation` to perform  Backward Mapping with bilinear Interpolation  using the function `map_coordinates()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc10b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import map_coordinates\n",
    "\n",
    "\n",
    "\n",
    "# Define transformation parameters\n",
    "angle_deg = -30.6       # Rotation angle in degrees\n",
    "tx = 22.3              # Translation along x\n",
    "ty = 10.15             # Translation along y\n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "T = np.array([\n",
    "    [cos_a, -sin_a, tx],\n",
    "    [sin_a, cos_a, ty],\n",
    "    [0,0,1]\n",
    "]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_backward_bilinear(image, T, order =1):\n",
    "    \"\"\"\n",
    "    Transformation method in order to apply structural transformation on an image\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image (NumPy array)\n",
    "    - T: Matrix of the structural transformation\n",
    "\n",
    "    Returns:\n",
    "    - Transformed image\n",
    "    \"\"\"\n",
    "    # 1. Récupérer les lignes et les colonnes, calculer les centres\n",
    "    rows, cols = image.shape\n",
    "    center_x, center_y = cols / 2, rows / 2\n",
    "\n",
    "    # 2. Créer l'image et calcul l'inverse de la transformation\n",
    "    output_image = np.zeros_like(image)\n",
    "    T_inv = np.linalg.inv(T)\n",
    "\n",
    "    # 3. Pour chaques pixels\n",
    "    for y_out in range(rows):\n",
    "        for x_out in range(cols):\n",
    "            # a. Coordonées centrées de l'image de sortie\n",
    "            x_centered_out = x_out - center_x\n",
    "            y_centered_out = y_out - center_y\n",
    "\n",
    "            # b. Appliquer la transformation inverse\n",
    "            src_x = T_inv[0,0]*x_centered_out + T_inv[0,1]*y_centered_out + T_inv[0,2]\n",
    "            src_y = T_inv[1,0]*x_centered_out + T_inv[1,1]*y_centered_out + T_inv[1,2]\n",
    "\n",
    "            # c. Revenir à l'espace image\n",
    "            x_in, y_in = src_x + center_x, src_y + center_y\n",
    "\n",
    "            # d. Vérifier si les nouvelles coordonnées sont dans les limites\n",
    "            if 0 <= x_in < cols and 0 <= y_in < rows:\n",
    "                interpolated_value = map_coordinates(\n",
    "                image,\n",
    "                [[y_in], [x_in]],      # Coordinates in (row, col) order\n",
    "                order= order              # 1 = bilinear, 3 = cubic spline\n",
    "                )[0]\n",
    "                output_image[y_out,x_out] = interpolated_value\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_backward_bilinear = out_backward_bilinear(loaded_gt,T)\n",
    "\n",
    "# Create a 1x3 grid of subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "# Backward (NN) Mapping\n",
    "axes[0].imshow(output_backward_nn, cmap='gray')\n",
    "axes[0].set_title(\"Output_backward_NN\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Ground Truth\n",
    "axes[1].imshow(loaded_gt, cmap='gray')\n",
    "axes[1].set_title(\"loaded_ground_truth\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Backward (bilinear) Mapping\n",
    "axes[2].imshow(output_backward_bilinear, cmap='gray')\n",
    "axes[2].set_title(\"Output_backward_Bil\")\n",
    "axes[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_valBBiL = mse(output_backward_bilinear, loaded_gt)\n",
    "psnr_valBBiL = psnr(output_backward_bilinear, loaded_gt)\n",
    "ssim_valBBiL = ssim(output_backward_bilinear, loaded_gt)\n",
    "\n",
    "    \n",
    "#Given\n",
    "print(f\"{'Method':<20} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'backward NN':<20} | {mse_valBBiL:10.2f} | {psnr_valBBiL:10.2f} | {ssim_valBBiL:10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Given: Metrics for all experiments\n",
    "print(f\"{'Method':<20} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Forward NN':<20} | {mse_valFNN:10.2f} | {psnr_valFNN:10.2f} | {ssim_valFNN:10.4f}\")\n",
    "print(f\"{'Forward BiL':<20} | {mse_valFBiL:10.2f} | {psnr_valFBiL:10.2f} | {ssim_valFBiL:10.4f}\")\n",
    "print(f\"{'backward NN':<20} | {mse_valBNN:10.2f} | {psnr_valBNN:10.2f} | {ssim_valBNN:10.4f}\")\n",
    "print(f\"{'backward BiL':<20} | {mse_valBBiL:10.2f} | {psnr_valBBiL:10.2f} | {ssim_valBBiL:10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80d51d",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "As in the previous experiment, evaluate the results both **qualitatively** and **quantitatively**. Provide your observations on the output image in terms of:\n",
    "\n",
    "- Visual quality  \n",
    "- Structure preservation  \n",
    "- Any visible distortions  \n",
    "\n",
    "Then, compare these results with those obtained in the **first, second and third experiment**. Highlight key differences in terms of:\n",
    "\n",
    "- Accuracy  \n",
    "- Interpolation smoothness  \n",
    "- Artifact presence  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Quantitative Evaluation\n",
    "\n",
    "Compute and report the following metrics between the **original ground truth image** and the **transformed (mapped-back) result**:\n",
    "\n",
    "- **MSE (Mean Squared Error)**  \n",
    "- **PSNR (Peak Signal-to-Noise Ratio)**  \n",
    "- **SSIM (Structural Similarity Index)**  \n",
    "\n",
    "\n",
    "> _Replace the placeholders with your actual computed results.\n",
    "\n",
    "| Method                  | MSE       | PSNR (dB) | SSIM     |\n",
    "|-------------------------|-----------|-----------|----------|\n",
    "| Forward Mapping NN      | 2015.57   | 15.09     | 0.2883   |\n",
    "| Forward Mapping Bilinear| 1247.23   | 17.17     | 0.6183   |\n",
    "| Backward Mapping NN     | 6050.76   | 10.31     | 0.4077   |\n",
    "| Backward Mapping BiL    | 6024.56   | 10.33     | 0.4149   |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Qualitative Evaluation\n",
    "\n",
    "#### Visual Inspection Checklist:\n",
    "\n",
    "- **Sharpness** of details  \n",
    "- Presence or absence of **aliasing or blocky artifacts**  \n",
    "- How well **edges and fine structures** are preserved  \n",
    "- Any **blurring** introduced due to interpolation  \n",
    "\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- Summarize your findings here. Comment on which method is more suitable and in what context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e81fa6",
   "metadata": {},
   "source": [
    "On observe que l'utilisation de l'interpolation bilinéaire n'affecte pas beaucoup les mesures, étant donnée qu'elle ne peut pas récupérer les 'dead-pixels\". On observe tout de même une légère amélioration, que ce soit sur le MSE ou sur le SSIM: l'amélioration du MSE est surement due au fait que les pixels sur le bord (initialement noir) vont être pondérés par les pixels ayant une intensité non-nul, réduisant ainsi un peu l'erreur. Ce qui change également un peu le contraste et donc améliore un peu le SSIM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d148f",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c22a36",
   "metadata": {},
   "source": [
    "# Experiment 5: Bidirectional mapping: (A → B and B → A),\n",
    "\n",
    "1. Choose the algo developed in Experiment 4.\n",
    "2. Perform mapping from the image to the transoformation.\n",
    "3. Perform mapping from the transformation back to the image.\n",
    "4. Perform qualitative and quantitative evaluation, here your ground truth image changed, it is the original image itself.\n",
    "\n",
    "* **Note:** Measuring the similarity between the original and reconstructed data can tell you how consistent and efficient the mappings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e198179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A -> B\n",
    "\n",
    "# Define transformation parameters\n",
    "angle_deg = 30.6       # Rotation angle in degrees\n",
    "tx = 22.3              # Translation along x\n",
    "ty = 10.15             # Translation along y\n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "T = np.array([\n",
    "    [cos_a, -sin_a, tx],\n",
    "    [sin_a, cos_a, ty],\n",
    "    [0,0,1]\n",
    "]) \n",
    "\n",
    "# Construct image\n",
    "output_AtoB = out_backward_bilinear(image, T)\n",
    "output_AtoB_2 = out_backward_nn(image, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd192478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A -> B\n",
    "\n",
    "# Define transformation parameters\n",
    "angle_deg = 30.6       # Rotation angle in degrees\n",
    "tx = 22.3              # Translation along x\n",
    "ty = 10.15             # Translation along y\n",
    "\n",
    "# Convert angle to radians\n",
    "angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "# Calculate cosine and sine of angle\n",
    "cos_a = np.cos(angle_rad)\n",
    "sin_a = np.sin(angle_rad)\n",
    "\n",
    "T = np.array([\n",
    "    [cos_a, -sin_a, tx],\n",
    "    [sin_a, cos_a, ty],\n",
    "    [0,0,1]\n",
    "]) \n",
    "\n",
    "# Construct image\n",
    "output_BtoA = forward_bilinear(output_AtoB, T)\n",
    "output_BtoA_2 = out_forward_nn(output_AtoB_2, T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2428c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 8))\n",
    "\n",
    "# Show the original image\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Show the AtoBtoA image (Bil)\n",
    "axes[1].imshow(output_BtoA, cmap='gray')\n",
    "axes[1].set_title(\"A->B->A Mapping (Bil operator)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Show the AtoBtoA image (NN)\n",
    "axes[2].imshow(output_BtoA_2, cmap='gray')\n",
    "axes[2].set_title(\"A->B->A Mapping (NN operator)\")\n",
    "axes[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure differences\n",
    "mse_valAtoBtoA = mse(output_BtoA, image)\n",
    "psnr_valAtoBtoA = psnr(output_BtoA, image)\n",
    "ssim_valAtoBtoA = ssim(output_BtoA, image)\n",
    "\n",
    "mse_valAtoBtoA_NN = mse(output_BtoA_2, image)\n",
    "psnr_valAtoBtoA_NN = psnr(output_BtoA_2, image)\n",
    "ssim_valAtoBtoA_NN = ssim(output_BtoA_2, image)\n",
    "\n",
    "    \n",
    "#Given\n",
    "print(f\"{'Method':<20} | {'MSE':>10} | {'PSNR':>10} | {'SSIM':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'AtoBtoA':<20} | {mse_valAtoBtoA:10.2f} | {psnr_valAtoBtoA:10.2f} | {ssim_valAtoBtoA:10.4f}\")\n",
    "print(f\"{'AtoBtoA NN':<20} | {mse_valAtoBtoA_NN:10.2f} | {psnr_valAtoBtoA_NN:10.2f} | {ssim_valAtoBtoA_NN:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2595db",
   "metadata": {},
   "source": [
    "Nous obtenons un résultat très concluant pour la méthode avec l'opérateur bilinéaire, permettant ainsi de combler les trou grâce à l'interpolation et la pondération des pixels voisins pour les pixels manquants. Nous observons un MSE très important, du fait de la perte d'information lors des transformation (des pixels (noirs) hors du cadre de l'image de base se sont retrouver dans le cadre de l'image lors de la première transformation, ce qui est une perte d'information non-négligeable, impossible à rattraper lors de la deuxième opération).\n",
    "Les résultats pour la méthode avec l'opérateur NN sont quand à eux très faible. Nous pouvons cependant distinguer très nettement l'objet de départ (parrot), les algorithmes sont très simple à implémenter et ne nécessitent pas une grosse capacités de calculs. Pour des algorithmes demandant beaucoup de calculs (classification avec beaucoup d'epoch, beaucoup d'individus) cette méthode pourrait suffire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c89ad4",
   "metadata": {},
   "source": [
    "* **Your Feedback (Optional but highly recommended for improving the BE session)**\n",
    "\n",
    "  1. Was the session too long or too short?\n",
    "  2. Did you find it easy or difficult?\n",
    "  3. Was it engaging or uninteresting?\n",
    "  4. Did you feel adequately guided throughout the session?\n",
    "  5. What aspects could be improved?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9de4c",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db10ef5",
   "metadata": {},
   "source": [
    "La session était clairement trop courte pour faire le BE en entier, et ce BE nous a pris beaucoup plus de temps que les précédents. Il n'était pas compliqué, plutôt bien guidé, et basé sur les notions vu en cours. Le problème est qu'il est très répétitif, avec toujours les mêmes questions qui nous oblige à se répéter. L'expériment 5 est nettement plus intéressant, et permet de mieux visualiser les opération (et combinaison d'opération) que l'on effectue. Un aspect qui pourrait être amélioré serait de directement guider les étudiants à l'élaboration des 4 méthodes (forward NN / Bil et backward NN / Bil) afin de visualiser plus en détails les différences et d'analyser qu'une seule fois ces différences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
